# NLP_Assignment_5

# üìå Direct Preference Optimization (DPO) with Hugging Face

## üîç Overview
This project implements **Direct Preference Optimization (DPO)** using **Hugging Face Transformers** and **LoRA (Low-Rank Adaptation)** to fine-tune a **GPT-2** model on **human preference rankings** from the **Anthropic/hh-rlhf** dataset. 

The project includes:
1. **Training a preference-based model using DPOTrainer**
2. **Hyperparameter experimentation & performance evaluation**
3. **Uploading the trained model to Hugging Face Hub**
4. **Deploying a web application for real-time model interaction**

---

## üìö Task 1: Finding a Suitable Dataset
- **Dataset Used:** [Anthropic/hh-rlhf](https://huggingface.co/datasets/Anthropic/hh-rlhf)
- **Reason for Selection:** This dataset contains human preference data, making it ideal for reinforcement learning from human feedback (RLHF) tasks like **Direct Preference Optimization (DPO)**.
- **Preprocessing Steps:**
  - Extracted prompts, chosen, and rejected responses.
  - Tokenized and truncated data to fit the model‚Äôs maximum length of 1024 tokens.
  - Applied padding for uniformity.
  
---

## üèãÔ∏è Task 2: Training the Model with DPOTrainer
### **Model & Training Setup:**
- **Base Model:** `GPT-2` (Loaded from Hugging Face)
- **Fine-Tuning Approach:** Applied **LoRA (Low-Rank Adaptation)** to train a preference model efficiently.
- **Trainer:** Used `DPOTrainer` for reinforcement learning-based optimization.
- **Quantization:** Applied `BitsAndBytesConfig` for **4-bit precision**, reducing memory usage.

### **Hyperparameter Experiments & Performance**
We experimented with different learning rates and batch sizes:
| Learning Rate | Batch Size | Epochs | Final Loss |
|--------------|------------|--------|------------|
| 5e-07       | 1          | 3      | 0.676000   |
| 1e-06       | 2          | 3      | 0.698400   |

üîπ **Observations:**
- The loss decreased over time, indicating successful training.
- A higher batch size improved stability but required more memory.
- Lower learning rates resulted in smoother optimization.

---

## üöÄ Task 3: Uploaded Model on Hugging Face
The trained model has been uploaded to the Hugging Face Model Hub.

**üîó Model Link:** [https://huggingface.co/Lakhika997/fine-tuned-dpo-model](https://huggingface.co/Lakhika997/fine-tuned-dpo-model)

To use the model, refer to the link above.

---

## üåê Task 4: Web Application
A **Streamlit-based web application** has been developed to demonstrate the model‚Äôs capabilities.


### **How to Run the Web App**
1. **Install Dependencies:**
   ```bash
   pip install torch transformers peft streamlit
   ```
2. **Run the App:**
   ```bash
   streamlit run dpo_app.py
   ```
3. **Interact with the Model:**
   - Enter a text prompt.
   - Click `Generate Response`.
   - View the AI-generated response.


![image](https://github.com/user-attachments/assets/79b09e94-e80f-406d-83df-80d1e4f19c80)


### **Files in This Repository**
| File Name | Description |
|-----------|-------------|
| `dpo_notebook.ipynb` | Jupyter Notebook for training and evaluation |
| `dpo_app.py` | Streamlit Web App for model inference |
| `training_results.xlsx` | Hyperparameter tuning results |
| `README.md` | Documentation for this project |
| `web_app_screenshot.png` | Screenshot of the web application |

---

## üîó Additional Resources
- **Hugging Face Model Hub:** [Hugging Face](https://huggingface.co/)
- **DPOTrainer Documentation:** [Hugging Face Docs](https://huggingface.co/docs/transformers/main/en/model_doc/dpo)
- **LoRA (PEFT) Guide:** [Hugging Face PEFT](https://huggingface.co/docs/peft/index)

---


